{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WMqegZ_nVhd"
      },
      "source": [
        "1.   Implementing the Sliding Window Algorithm\n",
        "2.   Performing Vehicle Recognition Prediction on Sliding Windows\n",
        "3.   Exploring Convolutional Neural Networks\n",
        "4.   Exploring Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvEWc0qfOUQc"
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications import VGG16, VGG19, ResNet50, DenseNet121\n",
        "\n",
        "from PIL import Image\n",
        "import gdown\n",
        "from IPython import display\n",
        "\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/image-2.jpg\"\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/image2.jpg\"\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20%20Object%20Detection%20(Autonomous%20Vehicles)/sliding.gif.png\"\n",
        "\n",
        "# Show sliding windows\n",
        "def show_sliding_window():\n",
        "  return display.Image(filename=\"sliding.gif.png\")\n",
        "\n",
        "# Construct vehicle dataset\n",
        "label_car = 1\n",
        "label_truck = 9\n",
        "\n",
        "# Load data\n",
        "def load_cifar10():\n",
        "  (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n",
        "  y_train_cifar = y_train_cifar.squeeze()\n",
        "  y_test_cifar = y_test_cifar.squeeze()\n",
        "  return (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar)\n",
        "\n",
        "# CIFAR100 classes\n",
        "idx_to_class = ['background', 'car', 'truck']\n",
        "\n",
        "# Construct vehicle dataset from CIFAR10\n",
        "def construct_vehicle_dataset(data, labels, images_per_class, label_car=1, label_truck=9):\n",
        "  mask_car = labels == label_car\n",
        "  mask_truck = labels == label_truck\n",
        "\n",
        "  mask_vehicles = mask_car | mask_truck\n",
        "  mask_background = np.invert(mask_vehicles)\n",
        "\n",
        "  data_car = data[mask_car]\n",
        "  data_truck = data[mask_truck]\n",
        "  data_background = data[mask_background][:images_per_class]\n",
        "\n",
        "  new_data = np.vstack((data_background, data_car, data_truck))\n",
        "  new_labels = np.repeat(np.array([0, 1, 2]), images_per_class, axis=0)\n",
        "\n",
        "  return new_data, new_labels\n",
        "\n",
        "def load_vehicle_dataset():\n",
        "  (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = load_cifar10()\n",
        "  x_train, y_train = construct_vehicle_dataset(x_train_cifar, y_train_cifar, 5000)\n",
        "  x_test, y_test = construct_vehicle_dataset(x_test_cifar, y_test_cifar, 1000)\n",
        "  return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# plotting\n",
        "def plot_one_image(data, labels = [], index = None, image_shape = None, fig_size=None):\n",
        "  '''\n",
        "  if data is a single image, display that image\n",
        "\n",
        "  if data is a 4d stack of images, display that image\n",
        "  '''\n",
        "  ### cv2.imshow('image', data)\n",
        "  num_dims   = len(data.shape)\n",
        "  num_labels = len(labels)\n",
        "  if image_shape is not None:\n",
        "    target_shape = image_shape\n",
        "  else:\n",
        "    target_shape = (32, 32, 3)\n",
        "  # reshape data if necessary\n",
        "  if num_dims == 1:\n",
        "    data = data.reshape(target_shape)\n",
        "  if num_dims == 2:\n",
        "    data = data.reshape(np.vstack[-1, image_shape])\n",
        "  num_dims   = len(data.shape)\n",
        "\n",
        "  # check if single or multiple images\n",
        "  if num_dims == 3:\n",
        "    if num_labels > 1:\n",
        "      print('Multiple labels does not make sense for single image.')\n",
        "      return\n",
        "\n",
        "    label = labels\n",
        "    if num_labels == 0:\n",
        "      label = ''\n",
        "    image = data\n",
        "\n",
        "  if num_dims == 4:\n",
        "    image = data[index, :]\n",
        "    label = labels[index]\n",
        "\n",
        "  # plot image of interest\n",
        "  print('Label: %s'%label)\n",
        "  if fig_size is not None:\n",
        "    plt.figure(figsize=fig_size)\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "\n",
        "def model_to_string(model):\n",
        "  import re\n",
        "  stringlist = []\n",
        "  model.summary(print_fn=lambda x: stringlist.append(x))\n",
        "  sms = \"\\n\".join(stringlist)\n",
        "  sms = re.sub('_\\d\\d\\d','', sms)\n",
        "  sms = re.sub('_\\d\\d','', sms)\n",
        "  sms = re.sub('_\\d','', sms)\n",
        "  return sms\n",
        "\n",
        "def normalize(data):\n",
        "  # CIFAR100 mean (0.4914, 0.4822, 0.4465) std (0.2023, 0.1994, 0.2010)\n",
        "  return (data/255-np.array((0.4914, 0.4822, 0.4465))) / np.array((0.2023, 0.1994, 0.2010))\n",
        "\n",
        "def label_to_onehot(labels):\n",
        "  final_labels = np.zeros((len(labels), 3))\n",
        "  for i in range(len(labels)):\n",
        "    label = labels[i]\n",
        "    if label == 0:\n",
        "      final_labels[i,:] = np.array([1, 0, 0])\n",
        "    if label == 1:\n",
        "      final_labels[i,:] = np.array([0, 1, 0])\n",
        "    if label == 2:\n",
        "      final_labels[i,:] = np.array([0, 0, 1])\n",
        "  return final_labels\n",
        "\n",
        "def plot_acc(history, ax = None, xlabel = 'Epoch #'):\n",
        "  # i'm sorry for this function's code. i am so sorry.\n",
        "  history = history.history\n",
        "  history.update({'epoch':list(range(len(history['val_accuracy'])))})\n",
        "  history = pd.DataFrame.from_dict(history)\n",
        "\n",
        "  best_epoch = history.sort_values(by = 'val_accuracy', ascending = False).iloc[0]['epoch']\n",
        "\n",
        "  if not ax:\n",
        "    f, ax = plt.subplots(1,1)\n",
        "  sns.lineplot(x = 'epoch', y = 'val_accuracy', data = history, label = 'Validation', ax = ax)\n",
        "  sns.lineplot(x = 'epoch', y = 'accuracy', data = history, label = 'Training', ax = ax)\n",
        "  ax.axhline(0.333, linestyle = '--',color='red', label = 'Chance')\n",
        "  ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')\n",
        "  ax.legend(loc = 1)\n",
        "  ax.set_ylim([0.01, 1])\n",
        "\n",
        "  ax.set_xlabel(xlabel)\n",
        "  ax.set_ylabel('Accuracy (Fraction)')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def TransferClassifier_func(name, nn_params, trainable = True):\n",
        "  expert_dict = {'VGG16': VGG16,\n",
        "                  'VGG19': VGG19,\n",
        "                  'ResNet50':ResNet50,\n",
        "                  'DenseNet121':DenseNet121}\n",
        "\n",
        "  expert_conv = expert_dict[name](weights = 'imagenet',\n",
        "                                            include_top = False,\n",
        "                                            input_shape = nn_params['input_shape'])\n",
        "  for layer in expert_conv.layers:\n",
        "    layer.trainable = trainable\n",
        "\n",
        "  expert_model = Sequential()\n",
        "  expert_model.add(expert_conv)\n",
        "  expert_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "  expert_model.add(Dense(128, activation = 'relu'))\n",
        "  expert_model.add(Dropout(0.3))\n",
        "\n",
        "  expert_model.add(Dense(64, activation = 'relu'))\n",
        "\n",
        "  expert_model.add(Dense(nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "\n",
        "  expert_model.compile(loss = nn_params['loss'],\n",
        "                optimizer = optimizers.SGD(learning_rate=nn_params['learning_rate'], momentum=nn_params['momentum']),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return expert_model\n",
        "\n",
        "# neural net parameters\n",
        "image_shape          = (32, 32, 3)\n",
        "nn_params = {}\n",
        "nn_params['input_shape']       = image_shape\n",
        "nn_params['output_neurons']    = 3\n",
        "nn_params['loss']              = 'categorical_crossentropy'\n",
        "nn_params['output_activation'] = 'softmax'\n",
        "nn_params['learning_rate'] = 1e-3\n",
        "nn_params['momentum'] = 0.9\n",
        "\n",
        "TransferClassifier  = lambda name: TransferClassifier_func(name = name, nn_params = nn_params);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23LiYDriAaG_"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KEAmyV7TssG"
      },
      "source": [
        "Type of image we will be working with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AWyHSOqTrOC"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "image = np.asarray(Image.open('./image-2.jpg'))\n",
        "print(image.shape)\n",
        "plot_one_image(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFuqdZEuLJrK"
      },
      "source": [
        "#1. Implementing the sliding window algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdIXtdRjLWuu"
      },
      "source": [
        "One naive method is to apply the classifier on sliding windows of the input image to perform object detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X48ODv_mSC5v"
      },
      "source": [
        "show_sliding_window()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTjOW7oDVAoP"
      },
      "source": [
        "As you can see from the gif above, a sliding window technique is that we slide through the image, looking at cropped images of a fixed size.\n",
        "\n",
        "To implement this sliding window algorithm, let's first figure out how to crop an image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMe3tnYys0JN"
      },
      "source": [
        "Segmenting part of an image out, like below:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1EAw4z5a96OvL77jNAf9v6f7ItvUaGFGm)\n",
        "\n",
        "Implmentation in Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbqwpsGnuSqk"
      },
      "source": [
        "# first copy our image... Make sure to copy it! Otherwise, we may accidentally overwrite our original image.\n",
        "new_image = image.copy()\n",
        "print(new_image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BBdJfYhxTc-"
      },
      "source": [
        "We want to get a rectangle crop whose top left corner is at `(x, y)` of the image. The height and width of the cropping window is `window_h` and `window_w` respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_q6IxcAvHy0"
      },
      "source": [
        "x = 20\n",
        "y = 40\n",
        "window_h = 32\n",
        "window_w = 48\n",
        "\n",
        "plot_one_image(new_image[y:y+window_h, x:x+window_w])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GImmpjxHvIGn"
      },
      "source": [
        "The sliding windows are crops at multiple `(x, y)` positions. To get the sliding windows, we can just iterate through `(x, y)` positions and do the cropping as what we did above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUCyKGvE2RnK"
      },
      "source": [
        "step_h = 16\n",
        "step_w = 16\n",
        "window_h = 32\n",
        "window_w = 32\n",
        "\n",
        "windows = []\n",
        "for y in range(0, new_image.shape[0], step_h):\n",
        "  for x in range(0, new_image.shape[1], step_w):\n",
        "    window = new_image[y:y+window_h, x:x+window_w]\n",
        "    if window.shape[0]==window_h and window.shape[1]==window_w:\n",
        "      windows.append(window)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4rm9D5P_rdl"
      },
      "source": [
        "Recall that the label and class name mapping is\n",
        "```\n",
        "0 - background\n",
        "1 - car\n",
        "2 - truck\n",
        "```\n",
        "We have created labels for the windows in advance, as defined below. Now let's plot all the windows and their labels to check what you've got! Hopefully, the labels look reasonable to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFQmFRpAleW1"
      },
      "source": [
        "labels = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtbrIfII_rPY"
      },
      "source": [
        "for window, label in zip(windows, labels):\n",
        "  plot_one_image(window, [label], fig_size=(1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vEbIzDG8Tc5"
      },
      "source": [
        "Recall that the inputs to our neural network models are `numpy` arrays, we need to convert our `windows` list to a `numpy` array. You can do this using the function `numpy.stack()`, which joins a sequence of same dimension arrays along a new axis. Also check if the shape of the result array is what you expect!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chy86SDP3yX-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "windows = np.stack(windows)\n",
        "print(windows.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIiqjRnTd6XE"
      },
      "source": [
        "#2. Vehicle Recognition Prediction on Sliding Windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqw6LA2NYB-A"
      },
      "source": [
        "Once we have the patches of the cropped image saved in the numpy array `windows`, we can now apply the perceptron that we trained to recognize cars.\n",
        "\n",
        "Load the vehicle dataset, build the perceptron model and train it on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyCr6LrVeOU4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsuaGL9wai_p"
      },
      "source": [
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = load_vehicle_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtDHbgPxanCS"
      },
      "source": [
        "# Build model\n",
        "perceptron = Sequential()\n",
        "perceptron.add(Flatten(input_shape = (32, 32, 3)))\n",
        "perceptron.add(Dense(units = 128, activation = 'relu'))\n",
        "perceptron.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "perceptron.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.SGD(learning_rate=1e-3, momentum=0.9),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZdEl5pHatkz"
      },
      "source": [
        "# Preprocess data\n",
        "monitor = ModelCheckpoint('./model.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "\n",
        "x_train_norm = normalize(x_train)\n",
        "x_test_norm = normalize(x_test)\n",
        "\n",
        "y_train_onehot = label_to_onehot(y_train)\n",
        "y_test_onehot = label_to_onehot(y_test)\n",
        "\n",
        "# Train the model\n",
        "history = perceptron.fit(x_train_norm, y_train_onehot, epochs=20, validation_data=(x_test_norm, y_test_onehot), shuffle=True, callbacks=[monitor])\n",
        "\n",
        "plot_acc(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbWRLpsrbmxR"
      },
      "source": [
        "The perceptron model can recognize cars and trucks at around 95% accuracy on the training set and around 72% accuracy on the test set.\n",
        "\n",
        "Combine the sliding window algorithm and the object detector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Ds-d1FdxnU"
      },
      "source": [
        "Given the sliding windows `windows`, we want to get the `perceptron`'s predictions `pred_y` about whether each patch contains a car/truck or not. We also want to get the model's confidence `pred_prob` about the prediction.\n",
        "\n",
        "\n",
        "* `normalize` the input data before feeding it into the model\n",
        "* The output of the model is the predicted probability over the 3 classes: car, truck and others. For each input image, the model chooses the predicted class to be the one with the highest probability. That probability is regarded as the model's confidence on this prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4PNe3HWgvrt"
      },
      "source": [
        "windows_norm = normalize(windows)\n",
        "output = perceptron.predict(windows_norm)\n",
        "\n",
        "pred_y = np.argmax(output, axis=-1)\n",
        "pred_prob = np.max(output, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJqhterJkAGW"
      },
      "source": [
        "Results are now saved in `pred_y` and `pred_prob`. Plot the detected vehicles.\n",
        "\n",
        "Set a confidence `threshold`, so that only the predictions that have higher confidence than the given `threshold` will be plotted.\n",
        "\n",
        "Recall that the label and class name mapping is\n",
        "```\n",
        "0 - background\n",
        "1 - car\n",
        "2 - truck\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTsRzOF-hHpE"
      },
      "source": [
        "threshold = 0.6\n",
        "\n",
        "num_windows = windows.shape[0]\n",
        "for i in range(num_windows):\n",
        "  if pred_y[i]>0 and pred_prob[i]>threshold:\n",
        "    plot_one_image(windows[i], labels=[\" \".join([str(pred_y[i]), str(pred_prob[i])])], fig_size=(1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-PP7MzGshG1"
      },
      "source": [
        "We can also calculate the accuracy of our predictions by comparing the predictions and the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwuIupEpmDzo"
      },
      "source": [
        "np.mean(pred_y == labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR8N499LnAfH"
      },
      "source": [
        "In order to reuse the code easily, include the prediction, plotting and calculating accuracy in one function, which returns the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uVmYajWnjlm"
      },
      "source": [
        "def sliding_predictions(model, windows, threshold=0.6, labels=labels):\n",
        "  windows_norm = normalize(windows)\n",
        "  output = model.predict(windows_norm)\n",
        "\n",
        "  pred_y = np.argmax(output, axis=-1)\n",
        "  pred_prob = np.max(output, axis=-1)\n",
        "\n",
        "  num_windows = windows.shape[0]\n",
        "  for i in range(num_windows):\n",
        "    if pred_y[i]>0 and pred_prob[i]>threshold:\n",
        "      plot_one_image(windows[i], labels=[\" \".join([str(pred_y[i]), str(pred_prob[i])])], fig_size=(1,1))\n",
        "\n",
        "  return np.mean(pred_y == labels)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9CmrRkgT5ZS"
      },
      "source": [
        "# 3. Exploring Convolutional Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RehfGJJWZFeG"
      },
      "source": [
        "Convolutional Neural Networks tend to perform much better on images, compared with the perceptron model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMi7nEjaIeN4"
      },
      "source": [
        "Our convolutional neural network is specified like:\n",
        "\n",
        "```\n",
        "cnn = Sequential()\n",
        "cnn.add(Conv2D(64, (3, 3), input_shape=(__, __, __)))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(units = 128, activation = 'relu'))\n",
        "cnn.add(Dense(units = NUM_OUTPUTS, activation = 'softmax'))\n",
        "```\n",
        "\n",
        "We see that we have a 1 convolution layer that takes in our inputs, and then 2 dense layers. Overall this is a 3 layer network.\n",
        "\n",
        "After specifying the network, we can compile it and train it just like before. Note:\n",
        "* We want loss to be `'categorical_crossentropy'`\n",
        "* Our optimizer will be  `optimizers.SGD(learning_rate=1e-3, momentum=0.95)`\n",
        "* Our metric is `['accuracy']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNDBeK9K2ZzU"
      },
      "source": [
        "cnn = Sequential()\n",
        "cnn.add(Conv2D(64, (3, 3), input_shape=(32, 32, 3)))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(units = 128, activation = 'relu'))\n",
        "cnn.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "cnn.compile(loss = 'categorical_crossentropy', optimizer = optimizers.SGD(learning_rate = 1e-3, momentum = 0.9),\n",
        "            metrics = ['accuracy'])\n",
        "\n",
        "cnn.fit(x_train_norm, y_train_onehot, epochs = 20, validation_data = (x_test_norm, y_test_onehot), shuffle = True, callbacks = [monitor])\n",
        "plot_acc(cnn.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkQFmKRxefS3"
      },
      "source": [
        "Same as how we got a simple vehicle detector by applying the trained perceptron to classifying the sliding windows, we can apply our new model as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPc2rU9Bfzoe"
      },
      "source": [
        "acc = sliding_predictions(cnn, windows, threshold=0.9)\n",
        "print(\"The accuracy is {}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5scKiYAYE8g"
      },
      "source": [
        "\n",
        "# 4. Expert models: Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFtHOYI2AdSs"
      },
      "source": [
        "For our transfer learning, we're going to use models built upon the famous 'ImageNet' classification problem.\n",
        "\n",
        "In ImageNet, participants were challenged to build machine learning models that could distinguish 14 million images' categories, where there were > 20,000 categories available.\n",
        "\n",
        "Below, we see examples of 4 different categories.\n",
        "\n",
        "![](http://cs231n.github.io/assets/trainset.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-E_AiG-CFj0"
      },
      "source": [
        "One of the experts we can use is VGG 16. VGG 16 wasÂ a network that was allowed to study the 14 million images 74 times.\n",
        "\n",
        "After its studying, VGG 16 was able to guess something close to the real label (top-5 accuracy) better than a human can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvkajtdHAbzL"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/1600/0*V1muWIDnPVwZUuEv.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwj8o5X3D325"
      },
      "source": [
        "Using transfer learning, VGC16 will be adapted to suit our usecase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz_mVsECHvro"
      },
      "source": [
        "\n",
        "```\n",
        "transfer = TransferClassifier(name = 'VGG16')\n",
        "```\n",
        "\n",
        "The different models we will be observing:\n",
        "* `VGG16`\n",
        "* `VGG19`\n",
        "* `ResNet50`\n",
        "* `DenseNet121`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VB79BCx7tvg"
      },
      "source": [
        "transfer = TransferClassifier(name = 'VGG16')\n",
        "transfer.fit(x_train_norm, y_train_onehot, epochs = 20, validation_data = (x_test_norm, y_test_onehot), shuffle = True, callbacks = [monitor])\n",
        "plot_acc(transfer.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMTOXiLXE0OX"
      },
      "source": [
        "Let's now load up VGG-16. The \"want\" is convolution layers of the model - that is, the layers that are most responsible for giving the model its visual understanding. The 'Dense/Fully Connected (FC)' layers are thought to be more specific to the ImageNet challenge- as in, what is most responsible for providing the model with its ability to identify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvIzSJ6_FONj"
      },
      "source": [
        "vgg_expert = VGG16(weights = 'imagenet', include_top = False, input_shape = (32, 32, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XyVd6o6FsRX"
      },
      "source": [
        "Now, we're going to plug the VGG expert into a custom model. To do this, we do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMzqTaE7F25Q"
      },
      "source": [
        "vgg_model = Sequential()\n",
        "vgg_model.add(vgg_expert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5oAm86oF63h"
      },
      "source": [
        "We want to add custom layers to our model... specifically,\n",
        "* `GlobalAveragePooling2D()`\n",
        "* `Dense(1024, activation = 'relu')`\n",
        "* `Dropout(0.3)`\n",
        "* `Dense(512, activation = 'relu')`\n",
        "* `Dropout(0.3)`\n",
        "* `Dense(3, activation = 'softmax')`\n",
        "\n",
        "And finally compile it with\n",
        "* loss: `categorical_crossentropy`\n",
        "* optimizer: `optimizers.SGD(learning_rate = 1e-4, momentum = 0.95)`\n",
        "* metrics: `accuracy`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpDt946zGhCZ"
      },
      "source": [
        "vgg_model = Sequential()\n",
        "vgg_model.add(vgg_expert)\n",
        "\n",
        "# and then add our own layers on top of it\n",
        "vgg_model.add(GlobalAveragePooling2D())\n",
        "vgg_model.add(Dense(1024, activation = 'relu'))\n",
        "vgg_model.add(Dropout(0.3))\n",
        "vgg_model.add(Dense(512, activation = 'relu'))\n",
        "vgg_model.add(Dropout(0.3))\n",
        "vgg_model.add(Dense(3, activation = 'softmax'))\n",
        "\n",
        "# finally, we build the vgg model and turn it on so we can use it!\n",
        "vgg_model.compile(loss = 'categorical_crossentropy',\n",
        "          optimizer = optimizers.SGD(learning_rate=1e-3, momentum=0.9),\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGv1qzzdHCdR"
      },
      "source": [
        "#Train the model\n",
        "vgg_model.fit(x_train_norm, y_train_onehot, epochs = 20, validation_data = (x_test_norm, y_test_onehot), shuffle = True, callbacks = [monitor])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkrw6963HI1W"
      },
      "source": [
        "Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gpwNg62GGJs"
      },
      "source": [
        "plot_acc(vgg_model.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yYe5yvXkGIP"
      },
      "source": [
        "Apply the model for vehicle detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2qvs5RBSgi"
      },
      "source": [
        "acc = sliding_predictions(vgg_model, windows, threshold=0.9)\n",
        "print(\"The accuracy is {}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}